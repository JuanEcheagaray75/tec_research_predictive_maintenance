\documentclass{beamer}
\usetheme{Madrid}
\usecolortheme{default}
\setbeamertemplate{navigation symbols}{}
\usepackage[style=ieee,backend=biber]{biblatex}
\usepackage{graphicx}
\usepackage[font=scriptsize,labelfont=bf]{caption}
\usepackage{pgfpages}
\usepackage{amsmath}
\usepackage{csquotes}
\usepackage{algorithm,algorithmic}
\usepackage{dsfont}
\addbibresource{references.bib}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\graphicspath{{img/}}

\title{Optimizing Aircraft Engine RUL Prediction with Bayesian-Enhanced Interpretable ML and SHAP}
\author{Juan Echeagaray}
\institute{School of Engineering and Sciences \\ Instituto Tecnológico y de Estudios Superiores de Monterrey}
\date{\today}

\begin{document}
    \frame{\titlepage}

    \begin{frame}
        \frametitle{Agenda}
        \tableofcontents
    \end{frame}

    \section{Introduction}

        \begin{frame}{Introduction}{Introduction}
            \begin{columns}
                \begin{column}{0.5\textwidth}
                    Maintenance scheduled by data analytics on historical data
                    \begin{itemize}
                        \item Enhance operational efficiency
                        \item Sustainability and cost reduction
                        \item Competitive advantage
                        \item \textbf{Safety at the forefront}
                    \end{itemize}
                \end{column}
                \begin{column}{0.5\textwidth}
                    \begin{figure}[!htbp]
                        \centering
                        \includegraphics[scale=0.3]{turbine_failure.jpg}
                        \caption{Turbine failure mid-flight \cite{bbc-news-2021}}
                    \end{figure}
                \end{column}
            \end{columns}
        \end{frame}


        \begin{frame}{Introduction}{Problem Statement}

            Predictive maintenance faces 2 main challenges:
            \begin{block}{Implementation}
                Need a model to estimate the RUL of a set of machinery, in an efficient and reliable manner
            \end{block}

            \begin{alertblock}{Interpretability}
                Need to \textit{understand} why a model produces a given RUL estimate, becomes more important in critical environments
            \end{alertblock}

        \end{frame}

    \section{Objectives}

        \begin{frame}
            \frametitle{Objectives}
            \begin{columns}
                \begin{column}{0.45\textwidth}
                    This research project aims to develop:
                    \begin{enumerate}
                        \item PdM framework to predict RUL for a designated fleet of machinery
                        \item Model which ensures reproducibility, stability, robustness and confidence
                        \item Tools to interpret and visualize the model's predictions
                    \end{enumerate}
                \end{column}
                \begin{column}{0.55\textwidth}
                    \begin{figure}[!htbp]
                        \centering
                        \includegraphics[scale=0.04]{uniform_fleet.jpg}
                        \caption{Sample of a uniform fleet of cars}
                    \end{figure}
                \end{column}
            \end{columns}
        \end{frame}

        \begin{frame}{Objectives}{Scope}
            \begin{columns}
                \begin{column}{0.5\textwidth}
                    The previous objectives are to be accomplished subject to the following constraints and assumptions:
                    \begin{itemize}
                        \item RUL prediction of an uniform fleet of machines
                        \item Availability of a labeled dataset with run to failure sequences of each machine
                    \end{itemize}
                \end{column}
                \begin{column}{0.5\textwidth}
                    \begin{figure}[!htbp]
                        \centering
                        \includegraphics[scale=0.4]{machine_efficiency.png}
                        \caption{Efficiency loss as a machine operates}
                    \end{figure}
                \end{column}
            \end{columns}
        \end{frame}

        \begin{frame}{Objectives}{Loss Function}
            The 2021 PHMAP challenge proposed the loss function \eqref{eqn:phmap_loss}; the average of the RMSE and NASA's scoring function \cite{saxena2008damage}.
            \begin{equation} \label{eqn:phmap_loss}
                \begin{gathered}
                    \mathcal{L}(y, \hat{y}) = \frac{1}{2} \left(\sqrt{\frac{1}{m}\sum_{i=1}^{m} (y_i - \hat{y}_i)^{2}} + \frac{1}{m}\sum_{i=1}^{m} \exp (\alpha \cdot (y_i - \hat{y}_i)) - 1 \right) \\
                    \alpha = \begin{cases}
                        \frac{-1}{10} & \text{if } y_i - \hat{y}_i \leq 0 \\
                        \frac{1}{13} & \text{if } y_i - \hat{y}_i > 0
                    \end{cases}
                \end{gathered}
            \end{equation}

            As a remark, \eqref{eqn:phmap_loss} is an asymmetric loss function with a higher penalty for overestimates.
        \end{frame}

    \section{Exploratory Data Analysis}

        \begin{frame}{Exploratory Data Analysis}{NCMAPSS Dataset}

            Flight conditions and readings from a fleet of turbofan engines,
            derived from NASA's CMAPSS model, including real flight conditions and relates the degradation process to the operating history of the machine. \cite{arias2021aircraft}
            \begin{itemize}
                \item Used in the PHMAP 2021 Data Challenge \cite{phm-conference}
                \item State of the art prognosis dataset (akin to MNIST and CIFAR for CV)
            \end{itemize}

            \begin{figure}[!htbp]
                \centering
                \includegraphics[scale=0.35]{cmapss_turbofan.png}
                \caption{CMAPSS turbofan engine schematic}
            \end{figure}
        \end{frame}

        \begin{frame}{Exploratory Data Analysis}{Data Overview}
            \begin{columns}
                \begin{column}{0.4\textwidth}
                    \begin{itemize}
                        \item Split into 10 h5 files
                        \item Sampling frequency of 1 second
                        \item Contains sensor readings, environmental descriptors, auxiliary variables, virtual sensor readings and RUL values
                        \item Corrupted file in the dataset
                    \end{itemize}
                \end{column}
                \begin{column}{0.6\textwidth}
                    \begin{table}[!htbp]
                        \centering
                        \scalebox{0.5}{
                        \begin{tabular}{ |l|l|l| }
                            \hline
                            Symbol & Description & Units \\
                            \hline
                            alt & Altitude & ft \\
                            Mach & Flight Mach number & - \\
                            TRA & Throttle-resolver angle & \% \\
                            T2 & Total temperature at fan inlet & °R \\
                            \hline
                            Wf & Fuel flow & pps \\
                            Nf & Physical fan speed & rpm \\
                            Nc & Physical core speed & rpm \\
                            T24 & Total temperature at LPC outlet & °R \\
                            T30 & Total temperature at HPC outlet & °R \\
                            T48 & Total temperature at HPT outlet & °R \\
                            T50 & Total temperature at LPT outlet & °R \\
                            P15 & Total pressure in bypass-duct & psia \\
                            P2 & Total pressure at fan inlet & psia \\
                            P21 & Total pressure at fan outlet & psia \\
                            P24 & Total pressure at LPC outlet & psia \\
                            Ps30 & Static pressure at HPC outlet & psia \\
                            P40 & Total pressure at burner outlet & psia \\
                            P50 & Total pressure at LPT outlet & psia \\
                            \hline
                            RUL & Remaining Useful Life & cycles \\
                            \hline
                            unit & Unit number & - \\
                            cycle & Flight cycle number & - \\
                            Fc & Flight class & - \\
                            hs & Health state & - \\
                            \hline
                        \end{tabular}}
                        \label{tab:dataset_info}
                        \caption{General description of dataset variables  \cite{phm-conference}}
                    \end{table}
                \end{column}
            \end{columns}

        \end{frame}

        \begin{frame}{Exploratory Data Analysis}{Flight Class Distribution}
            \begin{figure}[!htbp]
                \centering
                \includegraphics[scale=0.4]{fligt_class_per_unit_DS07.h5.png}
                \caption{Flights recorded for each class per unit}
            \end{figure}
            Flight classes encode flight duration\footnotemark[1], and there are no inter-class units
            \footnotetext[1]{FC1 ranges between 1 and 3 hours, FC2 between 3 and 5, and FC3 is greater than 5}
        \end{frame}

        \begin{frame}{Exploratory Data Analysis}{Sample Operating Conditions}
            \begin{figure}[!htbp]
                \centering
                \includegraphics[scale=0.35]{operating_conditions_unit_1_cycle_1.png}
                \caption{Operating conditions for the first flight of Unit 1}
            \end{figure}
        \end{frame}

        \begin{frame}{Exploratory Data Analysis}{Operating Conditions per Flight Class}
            \begin{figure}[!htbp]
                \centering
                \includegraphics[scale=0.35]{features_per_condition_fc.png}
                \caption{Distribution of operating conditions per Flight Class}
            \end{figure}

            Distinctive operating conditions per FC where FC2 can be seen as a mixture of FC1 and FC3
        \end{frame}

        \begin{frame}{Exploratory Data Analysis}{Final Sensor Readings}
            \begin{figure}[!htbp]
                \centering
                \includegraphics[scale=0.35]{unit_2_P2_cycles_82_84.png}
                \caption{Total Pressure at fan inlet (P2) for the last 3 cycles of unit 2}
            \end{figure}
            Smoothness degradation with a presence of new plateaus near the end of the flight
        \end{frame}

    \section{Methodology}

        \begin{frame}{Methodology}{Proposed Methodology}
            \begin{figure}[!htbp]
                \centering
                \includegraphics[scale=0.2]{research_diagram.png}
                \caption{Proposed Methodology}
            \end{figure}
        \end{frame}

        \subsection{Data Resampling and Downcasting}

            \begin{frame}{Data Resampling and Downcasting}
                \begin{columns}
                    \begin{column}{0.45\textwidth}
                        Sensor readings every second can lead to large arrays, think about sampling frequency (1 Hz, 1 KHz, 1 MHz, etc\dots) and data types.
                        \begin{itemize}
                            \item Resampling to catch the general shape
                            \item Downcasting to a datatype with lower memory footprint (64bit float to 32bit float)
                        \end{itemize}
                    \end{column}
                    \begin{column}{0.5\textwidth}
                        \begin{figure}[!htbp]
                            \centering
                            \includegraphics[scale=0.4]{resampling_series.png}
                            \caption{Time Series Resampling and Downcasting Effect}
                        \end{figure}
                    \end{column}
                \end{columns}
            \end{frame}

        \subsection{Time Series Segmentation}

            \begin{frame}{Time Series Segmentation}

                \begin{alertblock}{Problem}
                    Dataset consists of a set of multivariate time series of varying length with a distinct number of events per series
                \end{alertblock}
                \begin{figure}[!htbp]
                    \centering
                    \includegraphics[scale=0.3]{operating_conditions_unit_1_cycle_1.png}
                    \caption{Operating conditions of Plane 1 during its first flight}
                \end{figure}

                \note[item]{Altitude almost always seems to have 3 states, ascent, plateau, descent}
                \note[item]{Mechanical operations vary throughout the flight, there's no smooth increase or decrease, rather multiple operating stages}
                \note[item]{Take advantage of the syncrhonization between time series, look at plateau}
            \end{frame}

            \begin{frame}{Time Series Segmentation}{Basics}
                For a given signal $Y = \{y_t\}_{t=1}^{t=T}$ of $T$ samples, where $y_t \in \mathbb{R}^d$, we assume there exists a set $\mathcal{T} = \{t_{1}^{*}, t_{2}^{*}, \dots, t_{n}^{*}\}$ coding the $n-1$ stages of $Y$

                \begin{figure}[!htbp]
                    \centering
                    \includegraphics[scale=0.35]{show_series.png}
                    \caption{Piecewise constant signal with normal noise}
                    \footnote{The last index eases the software implementation}
                \end{figure}
            \end{frame}

            \begin{frame}{Search Algorithms}{Binary Segmentation}
                Greedy sequential algorithm that estimates 1st change-point as:
                \begin{equation}
                    \hat{t}_{1} := \argmin_{1 \leq t < T - 1} C(y_{0 \dots t}) + C(y_{t \dots T})
                \end{equation}

                It then repeats the operation on both left and right segments until the specified number of splits is achieved.

                \begin{figure}[!htbp]
                    \centering
                    \includegraphics[scale=0.3]{bin_seg_schematics.png}
                    \caption{Overview of Binary Segmentation retrieved from \cite{truong2020selective}}
                \end{figure}
            \end{frame}

            \begin{frame}{Cost Functions}{L2 Norm}
                An estimator of shifts in the central point of a distribution. Given a series $\{y_t\}_{t \in \mathcal{I}}$ where  $y_t \in \mathbb{R}^d$:
                \begin{equation}
                    C(y_{\mathcal{I}}) = \sum_{d} \sum_{t \in \mathcal{I}} ||y_t - \bar{y} ||_{2}^{2}
                \end{equation}

                where $\bar{y}$ is the component wise mean of $y_{\mathcal{I}}$

                \begin{itemize}
                    \item Needs scaling, considering not to penalize on segment length
                \end{itemize}

            \end{frame}

            \begin{frame}{Feature Extraction}{Statistical Descriptors}
                Given a segment $S_{i}$ which encompasses $T_i$ timestamps, $S_i \in \mathcal{M}_{d}^{T_i}(\mathbb{R})$ with $T_i \geq \alpha$ where $\alpha$ is a constraint on the minimum size of $S_i$. We then use a set of descriptors $F_s = \{f \mid  f: \mathcal{M}_{d}^{T_i}(\mathbb{R}) \rightarrow \mathbb{R}^{d}\}$ as an example:
                \begin{table}[!htbp]
                    \begin{tabular}{ll}
                    \multicolumn{2}{c}{Statistical Descriptors} \\ \hline
                    Minimum & Std. Deviation \\
                    25th Percentile & Variance \\
                    Median & Kurtosis \\
                    75th percentile & Skew \\
                    Maximum & Coeff. of Variation \\
                    Mean & - \\ \hline
                    \end{tabular}
                \end{table}

                Applying each element of $F_s$ onto $S_i$ results in a feature vector $\hat{S}_i \in \mathbb{R}^{d \cdot |F_s|}$
            \end{frame}

            \begin{frame}{Data Reduction}

                A flight $F_i \in \mathcal{M}_{d}^{T_i}(\mathbb{R})$ contains $d \cdot T_i$ elements, the feature vector $\hat{F}_i \in \mathbb{R}^{d |F_s| \tau}$ contains $ d |F_s| \tau$ elements, where $|F_s|$ is the number of statistical descriptors used and $\tau$ is the number of segments chosen.

                As an example, use a flight with 5,000 timestamps, 10 features, 10 statistical descriptors and 4 segments:
                \begin{gather}
                    | F_i | = 10 \cdot 5,000 = 50,000 \\
                    | \hat{F}_i | = 10 \cdot 10 \cdot 4 = 400 \\
                    \frac{ | F_i | }{ | \hat{F}_i | } = 125 \label{eq:data-reduction}
                \end{gather}
            \end{frame}

            \begin{frame}{Methodology}{Pros \& Cons}
                \begin{columns}
                    \begin{column}{0.45\textwidth}
                        \begin{exampleblock}{Pros}
                            \begin{itemize}
                                \item Large data reduction, see \eqref{eq:data-reduction}
                                \item Lower computational load
                                \item Ease feature interpretation
                                \item Reproducible
                            \end{itemize}
                        \end{exampleblock}
                    \end{column}
                    \begin{column}{0.45\textwidth}
                        \begin{alertblock}{Cons}
                            \begin{itemize}
                                \item Mixed or uniform MVT segmentation?
                                \item Not a fixed number of stages in each flight
                                \item Which descriptors to use?
                            \end{itemize}
                        \end{alertblock}
                    \end{column}
                \end{columns}
            \end{frame}

        \subsection{RUL Prediction}

            \begin{frame}{RUL Prediction}{Interpretable ML models}
                Given the reduced dataset, one now must select a model to estimate RUL. But how can you choose without prior information?
                \begin{enumerate}
                    \item Define a common loss function
                    \item Apply data transformation techniques \footnote{Any transformation must also be interpretable}
                    \item Select a suite of \textbf{easy to train} ML models
                    \item Baseline testing (w/o tuning) and ranking
                    \item Candidate model selection
                    \item Repeat!
                \end{enumerate}
            \end{frame}

        \subsection{Model Interpretation}
            \begin{frame}{Model Interpretation}{Importance and Benefits}

                \begin{columns}
                    \begin{column}{0.5\textwidth}
                        Interpretability enables \textbf{transparency} and \textbf{accountability}
                        \begin{itemize}
                            \item Analyze the weights of each feature in linear regression
                            \item Check the splits in a decision tree
                            \item Marginal changes in features affecting log-odds for logistic regression
                            \item Shapley and LIME for \enquote{higher models}
                        \end{itemize}
                    \end{column}
                    \begin{column}{0.5\textwidth}
                        \begin{figure}[!htbp]
                            \centering
                            \includegraphics[scale=0.15]{amazon_sexist.png}
                            \caption{Amazon removes its recruitment algorithm due to gender bias towards men \cite{reporter-2018}}
                        \end{figure}
                    \end{column}
                \end{columns}
            \end{frame}

        \begin{frame}{Model Interpretation}{Properties of Shapley values}

            \begin{itemize}
                \item \textbf{Efficiency:}\begin{equation*}
                    \sum_{j=1}^{p} \phi_j = \hat{f}(x) - \mathbb{E}[\hat{f}(X)]
                \end{equation*}
                \item \textbf{Symmetry:} \begin{equation*}
                    \begin{split}
                        val(S \cup \{j\}) & = val(S \cup \{k\}) \quad \forall S \subseteq \{1, \dots, p\} - \{j, k\} \\
                        & \Rightarrow \phi_j = \phi_k
                    \end{split}
                \end{equation*}
                \item \textbf{Dummy:}  \begin{equation*}
                    \begin{split}
                        val(S \cup \{j\}) & = val(S) \quad \forall S \subseteq \{1, \dots, p\} \\
                        & \Rightarrow \phi_j = 0
                    \end{split}
                \end{equation*}
                \item \textbf{Additivity:} \begin{equation*}
                    \phi_j^{f + g} = \phi_j^{f} + \phi_j^{g}
                \end{equation*}
            \end{itemize}
        \end{frame}

        \subsection{Prediction Intervals}

            \begin{frame}{Prediction Intervals}{Estimating uncertainty}

                \begin{columns}
                    \begin{column}{0.5\textwidth}
                        All predictions have an inherent level of uncertainty. \enquote{A prediction interval provides an estimate range within which a future observation is likely to fall} \cite{peters-2023}
                        \begin{exampleblock}{Machine Learning Perspective}
                            In ML, they represent the range within which a predicted observation is likely to fall, they estimate the uncertainty of point estimates.
                        \end{exampleblock}
                    \end{column}
                    \begin{column}{0.5\textwidth}
                        \begin{figure}[!htbp]
                            \centering
                            \includegraphics[scale=0.4]{sample_confidence_vs_prediction.png}
                            \caption{Difference between CIs and PIs \cite{cialdella-2020}}
                        \end{figure}
                    \end{column}
                \end{columns}
            \end{frame}

            \begin{frame}{Prediction Intervals}{Quantile Regression}

                Let $\tau$ be the selected quantile, $y$ the target variable and $\hat{y}$ the predicted quantile. The \textbf{Pinball Loss} is defined as:
                \begin{equation}\label{eqn:pinball}
                    \mathcal{L}_\tau(y, \hat{y}) = (y - \hat{y}) \tau \mathds{1} \left\{y \geq \hat{y}\right\} + (\hat{y} - y) (1 - \tau) \mathds{1} \left\{\hat{y} > y\right\}
                \end{equation}

                \begin{equation}\label{eqn:koenker_min}
                    \mathcal{Q}_\tau(Y | X) = \argmin_{q(X)} \mathbb{E}[\mathcal{L}_\tau(Y, q(X))]
                \end{equation}

                Koenker introduced the concept of quantile regression in 1978, and continued its development up to the Conditional Quantile Functions \cite{koenker2001quantile} referenced in \eqref{eqn:koenker_min} which minimizes \eqref{eqn:pinball}
            \end{frame}

        \subsection{Hyperparameter Optimization}

            \begin{frame}{Hyperparameter Optimization}{Overview}
                \begin{columns}
                    \begin{column}{0.5\textwidth}
                        Each dataset and model is different, and there is no rigorous proof on how to determine the optimal hyperparameters for a given problem. We adopt approximate methods divided into 3 categories \cite{aws-hyperparameter}:
                        \begin{itemize}
                            \item Grid Search
                            \item Random Search
                            \item \textbf{Bayesian Optimization}
                        \end{itemize}
                    \end{column}
                    \begin{column}{0.5\textwidth}
                        \begin{figure}[!htbp]
                            \centering
                            \includegraphics[scale=0.35]{hyperparameter_search.png}
                        \end{figure}
                    \end{column}
                \end{columns}
            \end{frame}

            \begin{frame}{Hyperparameter Optimization}{Tree-structured Parzen Estimators}

                Given a search history, it suggests a hyperparameter for the next trial.
                \begin{itemize}
                    \item Treats each hyperparameter \textbf{independently}
                    \item Processes search history as tuples of (parameter, loss)
                    \item Updates the definition of \enquote{good} and \enquote{bad} losses
                    \item Defines $g(x)$ and $b(x)$ for good and bad losses
                \end{itemize}

                \begin{exampleblock}{Heuristic}
                    Select the hyperparameter which maximices:
                    \begin{equation}
                        \begin{gathered}
                            S_g = \left\{X: X \sim g(x)\right\} \\
                            x_s = \argmax_{x \in S_g} \frac{g(x)}{b(x)}
                        \end{gathered}
                    \end{equation}
                \end{exampleblock}
            \end{frame}

            \begin{frame}{Hyperparameter Optimization}{Tree-structured Parzen Estimators Overview}
                \begin{figure}[placement]
                    \centering
                    \includegraphics[scale=0.8]{bayesian_algo_overview.png}
                    \caption{Example of TPE trial history \cite{pmlr-v139-tiao21a}}
                \end{figure}
            \end{frame}

    \section{Experimental Results}

        \begin{frame}{Segmentation of Environmental Descriptors}
            For a test case we apply Binary Segmentation with L2 norm and min. size for segment of 20\% the length of the array
            \begin{figure}
                \centering
                \includegraphics[scale=0.35]{environment_split_1_1.png}
                \caption{Environmental descriptors (scaled) for the first flight for plane 1}
            \end{figure}
        \end{frame}

        \begin{frame}{Flight History of a Plane}
            \begin{figure}
                \centering
                \includegraphics[scale=0.35]{DS01-005_8_Wf_var_coeff.png}
                \caption{Coefficient of variation of the Fuel Flow for Unit 8. Flew 89 flights}
            \end{figure}
            Could there be a correlation between the early spikes and the relatively early health decline?
        \end{frame}

        \begin{frame}{Baseline Regression Models}

            A suite of ML models is fit to 80\% of the resulting dataset, tested on the remaining 20\%
            \begin{table}[!htbp]
                \scalebox{0.6}{
                    \begin{tabular}{llllll}
                        \hline
                        Model & Train Loss & Test Loss & Train RMSE (cycles) & Test RMSE (cycles) & Training Time (s) \\
                        \hline
                        Catboost & 2.043735 & 5.321611 & 3.769342 & 9.399465 & 25.692841 \\
                        LightGBM & 2.636837 & 5.366961 & 4.823249 & 9.478604 & 2.475869 \\
                        XGBoost & 0.69788 & 5.726742 & 1.304671 & 10.0481 & 6.989698 \\
                        Random Forest & 2.086446 & 5.898664 & 3.837787 & 10.338893 & 223.095046 \\
                        Ridge & 6.340865 & 6.775478 & 11.033299 & 11.746066 & 0.043501 \\
                        Elastic Net & 6.888502 & 7.023012 & 11.872991 & 12.126897 & 0.029605 \\
                        Lasso & 6.933741 & 7.053696 & 11.937772 & 12.170086 & 0.028591 \\
                        Lasso Lars & 6.933741 & 7.053696 & 11.937772 & 12.170086 & 0.043695 \\
                        SVM & 6.878442 & 7.168354 & 11.893748 & 12.372685 & 12.91969 \\
                        Decision Tree & 0.0 & 9.472139 & 0.0 & 14.871512 & 3.39316 \\
                        Dummy (mean) & 16.77968 & 17.324815 & 23.688969 & 24.351877 & 0.000823 \\
                        Linear Regression & 5.142092 & 87743.231556 & 9.112421 & 14.664064 & 0.246407 \\
                        \hline
                    \end{tabular}
                }
            \end{table}

            \begin{enumerate}
                \item Gradient Boosting family achieves overall the same performance
                \item Linear models present less overfitting but comparatively lower accuracy
            \end{enumerate}
        \end{frame}

        \begin{frame}{Dimensionality Reduction?}
            PCA on raw descriptors needs 28 components to explain 99\% of the total variance:
            \begin{figure}
                \centering
                \includegraphics[scale=0.35]{pca_99_untransformed.png}
                \caption{Explained variance ratio per Principal Component added up to 99\%}
            \end{figure}

            \textbf{Scaled PCA needs 93}, nonetheless weights should be distributed fairly, doesn't imply better prediction performance!
        \end{frame}

        \begin{frame}{Effects on Model Accuracy subject to prior Standarization}

            Using a XGBoost regressor as a baseline model, the following treatments were tested on a validation dataset:
            \begin{table}[!htbp]
                \begin{tabular}{ll}
                    \hline
                    Transformation & Test Loss \\
                    \hline
                    Raw &  6.402025 \\
                    PCA &  5.726742 \\
                    PCA(0.99) &  6.963744 \\
                    Standardized PCA &  6.746479 \\
                    Standardized PCA(0.99) &  7.448121 \\
                    \hline
                \end{tabular}
                \caption{Comparison of PCA and Standardization Treatments}
            \end{table}

            PCA enables an overall improvement on model loss, but the model doesn't rely on the \textit{most} important components.
        \end{frame}

        \begin{frame}{TPE Hyperparameter Optimization}

            \begin{itemize}
                \item Optimizes the following models: RUL estimator, Lower and Upper bounds for  prediction intervals
                \item 500 trials per model
                \item Minimizes the 5-fold cross-validated test PHMAP loss
            \end{itemize}

            \begin{table}[!htbp]
                \centering
                \scalebox{0.7}{
                \begin{tabular}{ll}
                    \hline
                    Parameter & Range \\
                    \hline
                    Variance Threshold & $U(0.1, 1)$ \\
                    Learning Rate & $\exp (U(-4.5, 0))$ \\
                    Boosting Rounds & $U_\mathbb{Z}(100, 10000)$ \\
                    Max Depth & $U_\mathbb{Z}(2, 30)$ \\
                    Min Child Weight & $U_\mathbb{Z}(2, 50)$ \\
                    Subsample & $U(0.01, 1)$ \\
                    Gamma & $ U(0, 100)$ \\
                    Alpha & $ U(0, 100)$ \\
                    Lambda & $ U(0, 100)$ \\
                    \hline
                \end{tabular}}
                \caption{Proposed hyperparameter search space}
            \end{table}
        \end{frame}

        \begin{frame}{TPE Results}

            The total optimization procedure took \textit{around} 8 hours (considering all models).

            \begin{table}[!htbp]
                \centering
                \scalebox{0.8}{
                \begin{tabular}{lllll}
                    \hline
                    Parameter & Range & RUL & Lower Bound & Upper Bound\\
                    \hline
                    Variance Threshold & $U(0.1, 1)$ & 0.10208 & 0.1 & 0.9447 \\
                    Learning Rate & $\exp (U(-4.5, 0))$ & 0.02114 & 0.1163 & 0.042279 \\
                    Boosting Rounds & $U_\mathbb{Z}(100, 10000)$ & 1963 & 5859 & 3830 \\
                    Max Depth & $U_\mathbb{Z}(2, 30)$ & 30 & 15 & 20\\
                    Min Child Weight & $U_\mathbb{Z}(2, 50)$ & 29 & 41 & 38 \\
                    Subsample & $U(0.01, 1)$ & 0.72610 & 0.85454 & 0.79559 \\
                    Gamma & $ U(0, 100)$ & 62.667 & 0.12458 & 0.10927 \\
                    Alpha & $ U(0, 100)$ & 8.7213 & 18.8598 & 11.3874 \\
                    Lambda & $ U(0, 100)$ & 34.832 & 20.921385 & 82.70694\\
                    \hline
                    Loss & - & 5.6873 & 1.853 & 2.3995\\
                    \hline
                \end{tabular}}
                \caption{TPE results after 500 trials}
            \end{table}
        \end{frame}

        \begin{frame}{Residual Overview}
            \begin{columns}
                \begin{column}{0.45\textwidth}
                    Looking deeper into the RUL estimator:
                    \begin{itemize}
                        \item Residuals centered at 0
                        \item Relatively fat distribution (higher kurtosis than normal)
                        \item ECDF at 0 is 53\%
                    \end{itemize}
                \end{column}
                \begin{column}{0.55\textwidth}
                    \begin{figure}
                        \centering
                        \includegraphics[scale=0.4]{optimized_xgb_residuals.png}
                    \end{figure}
                \end{column}
            \end{columns}

            Residuals with zero mean are expected for models minimizing MSE, but \textit{maybe} for this scenario a positive mean would be better (model which underestimates)
        \end{frame}

        \begin{frame}{Sample Predictions}
            \begin{figure}[!htbp]
                \centering
                \includegraphics[scale=0.6]{preds_DS03-012_5.png}
            \end{figure}
        \end{frame}

        \begin{frame}{Sample Predictions}
            \begin{figure}[!htbp]
                \centering
                \includegraphics[scale=0.6]{preds_DS07_8.png}
            \end{figure}
        \end{frame}

        \begin{frame}{Model interpretation}

            Next we show the overall importance of each variable in RUL regression:
            \begin{figure}
                \centering
                \includegraphics[scale=0.35]{shap_summary.png}
                \caption{SHAP values for the Top 12 features (selected by the mean absolute value across all predictions)}
            \end{figure}
        \end{frame}

        \begin{frame}{Instance Explanation}
            We can also analyze in detail a single RUL estimate:
            \begin{figure}
                \centering
                \includegraphics[scale=0.35]{shap_waterfall_example.png}
                \caption{SHAP values for the Top 10 features}
            \end{figure}
        \end{frame}

        \begin{frame}{Instance Explanation}
            Shapley Values also allow to check for dependencies:
            \begin{figure}
                \centering
                \includegraphics[scale=0.35]{shap_dependence_flight_number.png}
                \caption{SHAP dependence plot for flight number (color coded by flight class)}
            \end{figure}
        \end{frame}

    \section{Conclusions}

        \begin{frame}{Conclusions}
            
        \end{frame}

    \section*{References}
        \begin{frame}[allowframebreaks]
            \frametitle{References}
            \printbibliography
        \end{frame}
\end{document}
